Large language models are getting bigger. They’re getting smarter. And they’re breaking benchmarks faster than we can meaningfully define them. This progress is undeniably impressive, but it's also revealing a deeper issue in how we evaluate intelligence. Every few months, a new benchmark surfaces—sometimes as a reasoning test, a long-context memory task, or a code generation challenge—and for a moment it feels like we’ve found a new ceiling. Then, within weeks, the frontier model surpasses it by a wide margin, and we move on to the next one. This rapid cycle of setting and shattering benchmarks begs a larger question: are we truly measuring intelligence, or are we just iterating over tasks?

## **Performance Isn't the Full Picture**

Part of the issue lies in how we define success. Most benchmarks today are geared toward performance metrics: how accurate, how efficient, how human-like a model's outputs are. But intelligence, at least how we understand it in humans, isn’t just about performing well. It’s about understanding. It's about the ability to explain, to reflect, to teach, and to connect abstract concepts in ways that enrich other minds. In that sense, one of the most underexplored yet critical benchmarks in AI might be something we've overlooked: explainability, not just as a transparency tool, but as a core capability of intelligence itself.

## **The Human Analogy**

To ground this idea, I keep going back to a common human experience—watching a soccer game and suddenly having the intuition that a goal is coming.

You can feel it. The rhythm of the match changes. The midfielders link up one pass after another with a precision they didn’t have just minutes ago. The ball stays pinned in the opponent’s half, ricocheting between players like a growing storm. The defenders are retreating just slightly too much, and the air feels heavier, tighter. It’s a sense of inevitability—not because anyone tells you, not because the score changes, but because the *pattern* has changed. You can feel the tension compressing, about to snap into a goal.

And yet, when you try to explain it—to put into words why you knew—a blankness appears. It’s not one thing. It’s everything. It’s a latent pattern that your mind has absorbed over thousands of hours of watching the game, compressed into a single gut feeling that defies easy articulation.

Most fans feel it. Few can articulate it. And the ones who can—the tacticians, the analysts—are the ones who see and understand the underlying geometry, the passing sequences, the spacing collapses that make a goal almost inevitable.

This distinction mirrors what we see in AI systems.

## **Learning to Feel, Not Just Compute**

In the early days of game-playing AI, value functions were handcrafted. In chess, models were told to assign scores based on piece count, control of the center, pawn structure. These were interpretable, symbolic systems with well-understood priorities. But when AlphaZero arrived, it discarded these human priors in favor of learning its own latent value functions through self-play. It began to “feel” the board in ways that were superhuman, evaluating positions with no hard-coded rules. This was a massive leap forward in capability, but it came at the cost of explainability. We had built a system that outperformed us but couldn’t tell us why.

This shift didn’t stop with games.

## **When GPT-3 Brought Ideas to Life**

Today’s large language models, especially in the GPT family, have pushed this principle even further. They aren’t trained to reason explicitly. They’re trained to compress language and ideas into high-dimensional latent spaces. But in doing so, they develop emergent abilities—reasoning, analogizing, planning. When GPT-3 powered the original release of ChatGPT, one of its most impressive and immediately useful traits was its ability to generate analogies. The analogies weren’t always perfect, but they helped people understand difficult topics in simpler terms. That single skill—taking something complex and mapping it to something more familiar—turned ChatGPT into a tool for learning. And that capability has only improved as models have grown more powerful. Today, millions of users rely on LLMs not just to get answers, but to understand them.

We are seeing the beginnings of explainability as a capability.

These models are starting to show that they can compress knowledge and then attempt to express that compression in language that humans can understand. But there is still a long way to go. Most of these explanations remain surface-level or post hoc. The model produces a solution and then rationalizes it, rather than giving a faithful account of the reasoning that led to the result. The next step—the benchmark I think we are missing—is to measure how well a model can explain the reasoning it actually used, grounded in its latent computations.

## **A Benchmark That Advances Science**

This matters for more than just transparency or safety. It matters for science.

If we could get a model like AlphaZero to not only play chess at a superhuman level but to *explain* its internal valuation of the board—to tell us why it favors certain formations or sacrifices—we might uncover patterns in chess that humanity has missed for centuries. We would not just witness brilliance. We would *learn* from it.

And that idea scales.

Imagine the same principle applied to soccer. Imagine if an AI, having absorbed millions of matches, could explain precisely the sequence of spatial dynamics and positional imbalances that signal a goal's imminent arrival, in ways even seasoned coaches had never seen before.

Now take it a step further.

Imagine a model that could compress its understanding of the physical universe and propose a unifying principle that ties together the four fundamental forces of physics. Not just proposing it, but explaining it in a way that passes through the rigors of scientific validation. Imagine it reframing complexity theory, proposing novel insights in cryptography, or even finding proofs to questions that have haunted mathematicians for centuries.

If a model could explain its compressed internal abstractions about the world, the impact would not just be economic. It would be transformational for science itself.

## **Intelligence Must Be Held to Account**

In scientific fields, we demand rigor. A theory isn’t accepted because it sounds plausible—it must be provable, traceable, and logically sound. Mathematics has proofs. Physics has equations. Biology has mechanisms. AI should be no different. If a model claims something is true, we should expect it to explain how and why, not just produce an answer but justify it in terms that hold up to scrutiny. These systems are beginning to form internal abstractions and beliefs. If they are to be called intelligent, they must also be held to the standards of intelligibility.

And if we succeed in building systems that can explain themselves—faithfully, clearly, and deeply—then the payoff will be enormous.

These models will not just automate tasks. They will become collaborators in discovery. Imagine an AI that could explain the latent structure of natural language, or propose a new structure for the Standard Model, or reframe our assumptions about time complexity in algorithms. Imagine not just using AI, but learning from it—because it understands the world differently, and can explain those differences in ways we had not imagined.

## **A Thought Worth Holding**

This is not a research paper. It is a thought—a framing. But it is one I believe is worth pursuing. Because if intelligence is defined only by what a model can do, we will miss the larger opportunity. The ability to compress knowledge is powerful. But the ability to explain that compression—to teach, to illuminate, to reveal what lies underneath—that is something we should be striving for. Not just for alignment. Not just for safety.

Because that is what it means to understand.

