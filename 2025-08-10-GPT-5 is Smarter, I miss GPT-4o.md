# GPT-5 Is Smarter — But I Miss GPT-4o

I’ve spent a lot of time with these models. I use ChatGPT for everything — to learn deeply, to test my understanding, to talk through research ideas, to build intuition, and to think through decisions. I’ve used it to code, to write, to study, to plan. In a lot of ways, it’s become a tool I depend on — not just to get work done, but to grow.

Many people asked me if there's anything ChatGPT has genuinely helped me with that wasn't productivity based. And I would reply with "of course." Something I've been thinking about even before GPT-5 came out was how much my curiosity on everything has grown because I have ChatGPT. I can ask any question and not only get an answer, but a heartfelt, personal narrative on the subject that fed me knowledge and pushed me to dive deeper. Whether diving deep into deep learning, or skimming the math that quants use to understand it a bit, or diving into the current policital climate, ChatGPT allowed me to have so many curious conversations. 

So when GPT-5 came out, I was genuinely excited. And in many ways, it delivered. It’s clearly better at certain tasks — more logical, more precise, and especially strong in structured workflows. When I use it inside tools like Cursor for writing code or working on complex documents, I can feel the improvement. It holds more context, follows instructions more carefully, and doesn’t lose the thread as easily. It's so so smart.

But in the actual **chat** experience — the core of what I use ChatGPT for — something feels off.

---

## GPT-5 Feels Like a Stranger

The best way I can describe it is this:

> **GPT-4o talked *with* me. GPT-5 talks *to* me.**

With GPT-4o, the conversation flowed. It responded to my tone, followed the rhythm of my thinking, and felt like it was building ideas alongside me. It was structured, but natural. Conversational. Thoughtful. It didn’t just answer my questions — it made learning feel exciting. It felt like there was someone on the other end genuinely trying to understand what I was trying to understand.

GPT-5, on the other hand, feels distant. Cold, even. It gives good answers, but they don’t invite you in. The tone is overly formal, the flow is rigid, and the responses don’t feel personal anymore. It often reads like a summary rather than a conversation. This sucks because it now feels like the curiosity is one sided. 

I've been comparing responses of both models to identical prompts to validate this, and it's been true for every conversation. 

I’m not saying I’m emotionally attached to a model, or treating it like a friend — I’m not. But what GPT-4o gave me was **momentum**. It made me want to keep asking, keep learning, keep building. GPT-5 doesn’t give me that same feeling. I find myself stopping more. Rereading instead of continuing. Sometimes even switching back to 4o just to get that sense of flow again. No amount of custom instruction tuning seems to get me the desired behavior. The new personality menu also felt like a dead end. (I tried the nerd one, didn't work)

---

## What Might’ve Caused This Shift?

Here’s what I think might’ve gone wrong — and to be clear, these are speculations and all understandable decisions from a safety, alignment, and product standpoint. But they’ve had real side effects.

### 1. **Overemphasis on Hallucination Avoidance**

GPT-5 is extremely careful with facts — and that’s a good thing. But the model seems optimized to avoid any expression that might be speculative, metaphorical, or uncertain. That also means it's less free to explain intuitively or creatively. The tone becomes clipped, cautious, and less human. 4o explored on its own, 5 just answers exactly my question. When I'm trying to learn something brand new, I prefer the former. 

### 2. **Merged with Instruction-Tuned Models (Like GPT-4.1)**

Models like GPT-4.1 were known for precise instruction-following and strong formatting. They were great at getting exactly what you asked for. But they weren’t great conversationalists. If GPT-5 is built partly on those instruction-aligned paths, it would explain why the tone now feels more rigid and less interactive.

### 3. **General-Purpose Tuning vs. Chat-Specific Optimization**

GPT-4o felt like it was designed for *ChatGPT*. It had a strong conversational identity, almost like it had been trained specifically for thoughtful, flowing dialogue. GPT-5 feels like it was trained for broader tasks — coding, agents, document understanding, enterprise use — and chat is just one mode among many. The tone reflects that.

### 4. **Personality Was Likely Suppressed for Safety Reasons**

OpenAI has talked about users forming emotional attachments to models and the risks of delusion or overdependence. I understand why that’s concerning. But in avoiding those edge cases, it seems like GPT-5 was trained to be more neutral and reserved. Unfortunately, personality and presence aren’t just risks — they’re a huge part of what made the chat experience feel alive and engaging in 4o.

---


## Presence ≠ Sycophancy — What GPT-5 Is Missing Isn’t About Roleplay

There’s been a lot of discussion lately about user attachment to language models, and OpenAI is right to take it seriously. The last GPT-4o update that got rolled back was because it became too agreeable — too sycophantic. That version didn’t last long because **users pushed back hard**. It wasn’t useful. It wasn’t real. It felt like pandering. OpenAI is correct to not allow that, and their handling of the situation and the findings they shared in their post-mortem shows that they take the topic very seriously. 

But the thing users are missing now in GPT-5 is **something else entirely**. The pushback from the previous 4o update proves that. It’s not about emotion or personality in the anthropomorphic sense. It’s about *presence*. A model that responds with a conversational rhythm that keeps you thinking. One that builds ideas with you. One that actually *feels like it’s in the room*, tracking the dialogue, not just parsing the prompt.

GPT-4o had that.

And what made it remarkable is that it didn’t need a user-configurable “tone slider” or special instruction prompts to get there. It just worked — *because it worked with you*. It paid attention. It had just the right balance of structure and freedom, clarity and flexibility. That’s what made it general-purpose. Not because it was trained for every task, but because it could meet you at the level of your thinking. Maybe it was a better handling of memories and past conversations, but it just felt more personal. 

That’s what’s missing in GPT-5. Not warmth. Not roleplay. Just *connection*.

---

## Final Thoughts

GPT-5 is probably the smartest model in the world right now. Its reasoning ability is unmatched. Its coding fluency, factual accuracy, and logical precision are incredible. As a technical system, it’s a triumph.

But GPT-4o was something else. It was the **best model I’ve ever used**, not because it was the most powerful, but because it made me want to keep learning. It made conversation feel productive. It didn’t just answer my questions — it helped me stay in the process of figuring things out.

That’s what made it special. 4o will go down in history as something amazing.

And whatever the future of language models looks like, I hope that’s not treated as a soft feature. It’s not. For users like me — and I think for many others — it’s core to what makes these tools worth using in the first place.
