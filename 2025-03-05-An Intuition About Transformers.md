Over the past few days, I’ve been trying to wrap my head around a question that’s been bothering me for a while: *How do transformers—like the ones behind ChatGPT—actually work?* I’ve been reading and listening to both sides of a growing debate. Some argue that large language models are the path to artificial general intelligence, while others insist that these models are just sophisticated parrots—brute-force machines that stitch together surface-level patterns without true understanding. I’ll admit, I leaned toward the second view. It felt like we were stumbling toward AGI not by solving intelligence, but by throwing more data and compute at the problem and hoping something sticks.

What really confused me was the gap between how these models are trained and how they behave. All they do, technically, is predict the next word. That’s it. Yet here I am, talking to ChatGPT, asking it for advice, getting it to solve math problems, debug code. How does a model that predicts one word at a time somehow “understand” me so well? Where does that intelligence come from?

While I was wrestling with that, I was also digging into older work in computer vision, like AlexNet. I remembered how researchers discovered that in deep image models, different layers specialize in different things. The first layers pick up on simple features like edges and textures. Deeper layers build on those to recognize shapes, and the last layers identify full objects—dogs, faces, stop signs. No one told the network to do this; it just happened as a result of training. That stuck with me, and it's something I'll come back to.

Another thing that stuck with me is the talk between Jensen Huang and Ilya Sutskever, where Ilya said his famous murder mystery example. Imagine you feed a language model an entire story, complete with characters, clues, red herrings, and a final scene where the detective turns and says: “The killer is…” And somehow, the model fills in the correct name. That example used to stump me. If the model is just predicting the next word, how could it possibly understand the plot well enough to solve a mystery?

At first, it didn’t click. But then I started thinking more carefully about what it really means to minimize prediction loss. If the model completes the sentence “Barack Obama was born in ___” with “Canada,” it gets penalized. If it says “Hawaii,” it gets rewarded. Over billions of examples, the model gradually adjusts its internal representations so that the correct words are more likely to appear. That means even something like **factual knowledge** isn’t something explicitly taught—it’s something that *emerges* from being penalized whenever a non-fact is predicted. It learns facts not because it wants to, but because it *has to* in order to minimize loss.

That’s when the bigger picture began to form. I realized that these models are learning more than just how to predict the next word. They’re learning language itself—its structure, its rules, its logic. And here’s the part that really changed the way I see everything: I realized there’s a **hierarchy of predictions** happening inside these models. The early layers are predicting things like what character might come next, or what the subword might be. Then the middle layers start predicting things like parts of speech, grammatical relationships, and sentence roles. The deeper you go, the more abstract the predictions become—predicting facts, predicting likely answers, predicting reasoning patterns, predicting how a story should end, or how code should be written. At the very top, the model is no longer just predicting the next word in a vacuum—it's predicting entire **semantic structures**, narrative arcs, mathematical solutions, or stylistic choices across completely different domains. And it learns to do all of that just to get better at next-word prediction. That realization hit me hard.

It reminded me of what I saw in AlexNet: a hierarchy, where simple parts build into complex wholes. But here, it wasn’t edges to objects—it was **predictions to meaning**. And then I came back to Ilya’s example. I stopped thinking of it as “just” next-word prediction. It’s not just that the model guesses the name of the killer—it’s that, over its layers, it has encoded thousands of murder mystery patterns. It has learned what kinds of clues usually matter. It’s learned to identify characters, weigh their motives, and map the structure of suspense and resolution. Through attention and depth, it learns to weight the most important clues, filter out irrelevant ones, and compress the narrative into something that aligns with a likely resolution. When it predicts “Jonathan,” it’s not randomly guessing. It’s arriving at that prediction because it has seen enough examples, compressed enough narrative patterns, and abstracted enough logical reasoning to make a probabilistic—but highly informed—leap. That’s not symbolic deduction in the classical sense. But it’s something *very close* to how we reason.

That’s when it finally clicked for me. I used to think that intelligence needed to be explicitly engineered—that models would need symbolic logic, planning modules, and world models hardcoded into them. But what I now realize is how exactly **intelligence can emerge as a side effect of prediction**—when the prediction task is rich enough, and the model deep enough, and the data broad enough.

**Prediction forces compression. Compression forces abstraction. Abstraction leads to emergent understanding.**

That’s how a language model becomes more than just a predictor. That’s how it begins to understand—and maybe how something like intelligence begins to take shape. Like most things on this blog, this hasn't been empirically tested, but it's a nice intuitive framework into how these transformers organize ideas, and why they work. 
